{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "10c4b5a1-f479-4739-8d2c-8aa9c43dba19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/adrish/dark-matter-halos-2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/adrish/miniconda3/envs/test/lib/python3.10/site-packages/IPython/core/magics/osm.py:417: UserWarning: This is now an optional IPython functionality, setting dhist requires you to install the `pickleshare` library.\n",
      "  self.shell.db['dhist'] = compress_dhist(dhist)[-100:]\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0,1,2,3'\n",
    "os.environ['XLA_PYTHON_CLIENT_PREALLOCATE'] = '0'\n",
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7e3dac5d-b7c8-4fff-b28f-2c558b4a5db1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-16 15:35:32.421379: W external/xla/xla/service/gpu/nvptx_compiler.cc:765] The NVIDIA driver's CUDA version is 12.0 which is older than the ptxas CUDA version (12.8.61). Because the driver is older than the ptxas version, XLA is disabling parallel compilation, which may slow down compilation. You should update your NVIDIA driver or use the NVIDIA-provided CUDA forward compatibility packages.\n",
      "/home/adrish/miniconda3/envs/test/lib/python3.10/subprocess.py:1796: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  self.pid = _posixsubprocess.fork_exec(\n",
      "/home/adrish/miniconda3/envs/test/lib/python3.10/subprocess.py:1796: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  self.pid = _posixsubprocess.fork_exec(\n",
      "2025-02-16 15:35:34,452\tINFO worker.py:1812 -- Started a local Ray instance. View the dashboard at \u001b[1m\u001b[32m10.210.1.81:8265 \u001b[39m\u001b[22m\n"
     ]
    }
   ],
   "source": [
    "from src.pipeline.pipeline import *\n",
    "\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "import ray\n",
    "import gzip\n",
    "import jax\n",
    "from jax import numpy as jnp\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import LogNorm, LinearSegmentedColormap\n",
    "from matplotlib.cm import ScalarMappable\n",
    "\n",
    "\n",
    "ray.init(dashboard_host=\"0.0.0.0\", ignore_reinit_error=True)\n",
    "rng = np.random.default_rng(seed=0)\n",
    "\n",
    "root_path = Path(\".\")\n",
    "data_path = root_path / \"data\"\n",
    "generated_data_path = root_path / \"generated_data_extended\"\n",
    "cache_dir = generated_data_path / \"cache\"\n",
    "cache_dir.mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8f0ee0f1-69fb-499a-87f0-a537d8434036",
   "metadata": {},
   "outputs": [],
   "source": [
    "taus = np.logspace(-0.8, 3, num=12, base=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f807b998-5233-401a-b177-a734db20432c",
   "metadata": {},
   "outputs": [],
   "source": [
    "FILE = 'halo_pointclouds_extended.pkl.gz'\n",
    "with gzip.open(data_path / FILE, \"r\") as f:\n",
    "    points_list, velocities_list, halos_sel = pickle.load(f)\n",
    "downsample_sizes = np.logspace(2, 7.2, num=12, base=2.0).round().astype(np.int32)\n",
    "n_trials = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c856e638-e259-45de-a01a-341323451370",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.subsampling.subsample import *\n",
    "\n",
    "ignore_taus = set()\n",
    "ignore_downsample_size = set()\n",
    "ignore_feats = False\n",
    "\n",
    "for downsample_size in downsample_sizes:\n",
    "    for tau in taus:\n",
    "        path = generated_data_path / f\"kmeans_subsampled_tau{tau}_n{n_trials}_s{downsample_size}.npz\"\n",
    "        if path.exists():\n",
    "            ignore_taus.add(tau.item())\n",
    "            ignore_downsample_size.add(downsample_size.item())\n",
    "\n",
    "if (generated_data_path / \"features_and_targets.npz\").exists():\n",
    "    ignore_feats = True\n",
    "\n",
    "if len(ignore_taus) != len(taus):\n",
    "    points_list = [preprocess_pointcloud(p) for p in points_list]\n",
    "    \n",
    "    downsampled_data = dict()\n",
    "    \n",
    "    for downsample_size in (set(downsample_sizes.tolist()) - ignore_downsample_size):\n",
    "        print(\"Generating Samples for Downsample Size:\", downsample_size)\n",
    "    \n",
    "        sampled_positions_all, sampled_velocities_all, sampled_weights_all = kmeans_downsample_points(\n",
    "            points_list, velocities_list, taus, downsample_size, n_trials, pbar=True\n",
    "        )\n",
    "    \n",
    "        for tau_idx, tau in enumerate(set(taus.tolist()) - ignore_taus):\n",
    "            sampled_positions = sampled_positions_all[tau_idx]\n",
    "            sampled_velocities = sampled_velocities_all[tau_idx]\n",
    "            sampled_weights = sampled_weights_all[tau_idx]\n",
    "    \n",
    "            filename = f\"kmeans_subsampled_tau{tau}_n{n_trials}_s{downsample_size}.npz\"\n",
    "            jnp.savez(\n",
    "                generated_data_path / filename,\n",
    "                points=sampled_positions,\n",
    "                weights=sampled_weights,\n",
    "                velocities=sampled_velocities,\n",
    "            )\n",
    "\n",
    "if not ignore_feats:\n",
    "    print(\"Writing Features and Targets\")\n",
    "    jnp.savez(\n",
    "        generated_data_path / \"features_and_targets.npz\",\n",
    "        **halos_sel\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "415c22a5-531a-4710-a418-a8cf3ade0e5a",
   "metadata": {},
   "source": [
    "### Running Experiment on different k-means sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "290b4360-2202-4b04-a296-20b40cf618a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "n_halos = 500\n",
    "\n",
    "# Generating the train-test split: keep fixed seed of 0\n",
    "rng = jax.random.PRNGKey(0)\n",
    "prop_train = 0.75\n",
    "prop_test = 0.25\n",
    "\n",
    "n_train = int(n_halos * prop_train)\n",
    "n_test = n_halos - n_train\n",
    "\n",
    "train_indices_path = generated_data_path / \"train_indices.txt\"\n",
    "test_indices_path = generated_data_path / \"test_indices.txt\"\n",
    "\n",
    "if not train_indices_path.exists():\n",
    "    train_indices = jax.random.choice(rng, n_halos, [n_train], replace=False)\n",
    "    np.savetxt(train_indices_path, np.array(train_indices), fmt=\"%i\")\n",
    "    print(\"[TRAIN] Generating new indices\")\n",
    "else:\n",
    "    train_indices = np.loadtxt(str(train_indices_path)).astype(np.int32)\n",
    "    \n",
    "if not test_indices_path.exists():\n",
    "    test_indices = jnp.array(\n",
    "        list(set(range(n_halos)) - set([ix.item() for ix in list(train_indices)]))\n",
    "    )\n",
    "    np.savetxt(test_indices_path, np.array(test_indices), fmt=\"%i\")\n",
    "    print(\"[TEST] Generating new indices\")\n",
    "else:\n",
    "    test_indices = np.loadtxt(str(test_indices_path)).astype(np.int32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bae9b7a9-8223-47c4-a1c1-10b2562eb521",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "downsampled_data = {\n",
    "    (downsample_size, tau): f\"kmeans_subsampled_tau{tau}_n{n_trials}_s{downsample_size}.npz\"\n",
    "    for downsample_size in downsample_sizes\n",
    "    for tau in taus\n",
    "}\n",
    "        \n",
    "        # Function to preprocess training data\n",
    "def preprocess_data(features_and_targets, indices, subsampled_data, mass_range=(-11.1, 101.3)):\n",
    "    log_m = np.log10(features_and_targets[\"Group_M_Crit200\"][indices])\n",
    "    sliced_mass_ix = indices[\n",
    "        np.argwhere((log_m > mass_range[0]) & (log_m < mass_range[1]))[:, 0]\n",
    "    ]\n",
    "\n",
    "    weights = subsampled_data[\"weights\"][sliced_mass_ix, 0]\n",
    "    points = subsampled_data[\"points\"][sliced_mass_ix, 0]\n",
    "    velocities = subsampled_data[\"velocities\"][sliced_mass_ix, 0]\n",
    "    processed_data = {\n",
    "        \"stellar_mass\": jnp.log10(features_and_targets[\"StellarMass\"][sliced_mass_ix] * 1e10 / 0.677),\n",
    "        \"concentration\": features_and_targets[\"SubhaloC200\"][sliced_mass_ix],\n",
    "        \"mass\": jnp.log10(features_and_targets[\"Group_M_Crit200\"][sliced_mass_ix]),\n",
    "        \"stellar_metallicity\": features_and_targets[\"StellarMetallicity\"][sliced_mass_ix],\n",
    "        \"star_formation_rate\": features_and_targets[\"StarFormRate\"][sliced_mass_ix],\n",
    "    }\n",
    "\n",
    "    return processed_data, weights, points, velocities\n",
    "    \n",
    "# Function to create the problem context\n",
    "def create_problem_context(data, weights, points, velocities, label):\n",
    "    return ProblemContext(\n",
    "        points=points,\n",
    "        weights=weights,\n",
    "        velocities=velocities,\n",
    "        masses=data[\"mass\"],\n",
    "        concentrations=data[\"concentration\"],\n",
    "        labels=data[label],\n",
    "    )\n",
    "\n",
    "# Function to evaluate and collect loss arrays\n",
    "def evaluate_loss(problem_context, hyperparams):\n",
    "    result_dict, loss_array = get_oat_losses(problem_context, hyperparams, inner_pbar=False)\n",
    "    return result_dict, loss_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a5dccd7c-2c0e-4105-9b4d-18e82322ccf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def truncate_colormap(cmap, min_val=0.1, max_val=0.9, n_colors=256):\n",
    "    new_cmap = LinearSegmentedColormap.from_list(\n",
    "        'truncated_cmap', cmap(np.linspace(min_val, max_val, n_colors))\n",
    "    )\n",
    "    return new_cmap\n",
    "\n",
    "def plot_variable_effect(loss_data, variable_values, n_neighbors, variable_name, xlabel, ylabel, title, save_path):\n",
    "    fig, ax = plt.subplots()\n",
    "    \n",
    "    # Create a colormap based on the variable values\n",
    "    colormap = truncate_colormap(plt.cm.Blues, 0.1, 0.9)  # You can choose any colormap (e.g., 'plasma', 'inferno', 'magma', 'cividis')\n",
    "    normalize = LogNorm(vmin=min(variable_values), vmax=max(variable_values))\n",
    "    \n",
    "\n",
    "    for idx, var_value in enumerate(variable_values):\n",
    "        mean_losses = np.mean(loss_data[idx], axis=0)\n",
    "        color = colormap(normalize(var_value))\n",
    "        ax.plot(n_neighbors, mean_losses, color=color)\n",
    "    \n",
    "    sm = ScalarMappable(cmap=colormap, norm=normalize)\n",
    "    sm.set_array([]) \n",
    "    cbar = plt.colorbar(sm, ax=ax)\n",
    "    cbar.set_label(f\"{variable_name} (log-scale)\")\n",
    "    \n",
    "\n",
    "    ax.legend(loc=\"upper right\")\n",
    "    ax.set(xlabel=xlabel, ylabel=ylabel, title=title)\n",
    "    ax.set_xscale(\"log\")\n",
    "    \n",
    "    # Save and close the plot\n",
    "    plt.savefig(save_path, bbox_inches=\"tight\")\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "89869509-b5b5-4328-a7ca-6c3e6d6d2752",
   "metadata": {},
   "outputs": [],
   "source": [
    "@ray.remote(num_gpus=1)\n",
    "def process_downsampled_data_item(\n",
    "    size,\n",
    "    filename,\n",
    "    train_indices,\n",
    "    generated_data_path,\n",
    "    fixed_hyperparams,\n",
    "    variable_value,\n",
    "    n_neighbors,\n",
    "    label\n",
    "):\n",
    "    print(\"Processing File: \", filename)\n",
    "    subsampled_data = jnp.load(generated_data_path / filename)\n",
    "    features_and_targets = jnp.load(generated_data_path / \"features_and_targets.npz\")\n",
    "    train_data, weights, points, velocities = preprocess_data(features_and_targets, train_indices, subsampled_data)\n",
    "    problem_context = create_problem_context(train_data, weights, points, velocities, label)\n",
    "\n",
    "    hyperparametrization = Hyperparametrization(\n",
    "        rescale_strategy=[\"unitless\"],\n",
    "        p=fixed_hyperparams[\"p\"],\n",
    "        q=fixed_hyperparams[\"q\"],\n",
    "        tau=1,\n",
    "        alpha_C=fixed_hyperparams[\"alpha_C\"],\n",
    "        alpha_M=fixed_hyperparams[\"alpha_M\"],\n",
    "        alpha_SLB=fixed_hyperparams[\"alpha_SLB\"],\n",
    "        n_neighbors=n_neighbors,\n",
    "    )\n",
    "\n",
    "    result_dict, loss_array = evaluate_loss(problem_context, hyperparametrization)\n",
    "    return size, result_dict, loss_array\n",
    "\n",
    "\n",
    "def generate_results(\n",
    "    train_indices,\n",
    "    downsampled_data,\n",
    "    hyperparam_ranges,\n",
    "    variable_name,\n",
    "    generated_data_path,\n",
    "    label\n",
    "):\n",
    "    # Fix other hyperparameters to their default values\n",
    "    ignore_variables = {variable_name, \"n_neighbors\"}\n",
    "    fixed_hyperparams = {key: values[0] for key, values in hyperparam_ranges.items() if key not in ignore_variables}\n",
    "    \n",
    "    # Extract unique tau values from downsampled_data keys\n",
    "    tau_values = sorted({key[1] for key in downsampled_data.keys()})\n",
    "    variable_values = tau_values if variable_name == \"tau\" else hyperparam_ranges[variable_name]\n",
    "    n_neighbors = hyperparam_ranges[\"n_neighbors\"]\n",
    "\n",
    "    loss_data = []\n",
    "    result_data = []\n",
    "    for idx, var_value in enumerate(variable_values):\n",
    "        # Check if cached results exist\n",
    "        (cache_dir / label).mkdir(exist_ok=True)\n",
    "        cache_file = (cache_dir / label) / f\"{variable_name}_{var_value}.npz\"\n",
    "        if cache_file.exists():\n",
    "            print(f\"Loading cached results for {variable_name} = {var_value}\")\n",
    "            cached_data = np.load(cache_file, allow_pickle=True)\n",
    "            losses_for_var = cached_data[\"losses_for_var\"]\n",
    "            result_dict_for_var = cached_data[\"result_dict_for_var\"]\n",
    "        else:\n",
    "            # Launch remote tasks\n",
    "            result_refs = [\n",
    "                process_downsampled_data_item.remote(\n",
    "                    size,\n",
    "                    filename,\n",
    "                    train_indices,\n",
    "                    generated_data_path,\n",
    "                    fixed_hyperparams,\n",
    "                    var_value,  # Pass tau or other variable value\n",
    "                    n_neighbors,\n",
    "                    label\n",
    "                )\n",
    "                for (size, tau), filename in downsampled_data.items()\n",
    "                if tau == var_value or variable_name != \"tau\"  # Match tau when processing tau\n",
    "            ]\n",
    "\n",
    "            # Collect results\n",
    "            results = ray.get(result_refs)\n",
    "            results.sort(key=lambda x: x[0])  # Sort by size\n",
    "            losses_for_var = [loss_array for _, _, loss_array in results]\n",
    "            losses_for_var = np.stack(losses_for_var)\n",
    "            result_dict_for_var = [result_dict for _, result_dict, _ in results]\n",
    "\n",
    "            # Cache the results\n",
    "            np.savez(cache_file, losses_for_var=losses_for_var, result_dict_for_var=result_dict_for_var)\n",
    "            print(f\"Cached results for {variable_name} = {var_value}\")\n",
    "\n",
    "        loss_data.append(losses_for_var)\n",
    "        result_data.append(result_dict_for_var)\n",
    "\n",
    "    return loss_data, result_data, variable_values, n_neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9d1525ac-45ec-48d3-8686-fd186535b3bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_results_minimum(\n",
    "    loss_data,\n",
    "    variable_values,\n",
    "    n_neighbors,\n",
    "    variable_name,\n",
    "    output_dir,\n",
    "    regression_type,\n",
    "    find_minimum=False\n",
    "):\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    # Find the minimum loss if required\n",
    "    min_idx = 0\n",
    "    min_loss = float('inf')\n",
    "    if find_minimum:\n",
    "        for idx, losses_for_var in enumerate(loss_data):\n",
    "            if np.min(losses_for_var) < min_loss:\n",
    "                min_loss = np.min(losses_for_var)\n",
    "                min_idx = idx\n",
    "        loss_data = loss_data[min_idx:(min_idx + 1)]\n",
    "        variable_values = variable_values[min_idx : (min_idx + 1)]\n",
    "        prefix = \"_minimum\"\n",
    "    else:\n",
    "        prefix = \"\"\n",
    "\n",
    "    # Save the plot for the given regression type\n",
    "    plot_path = os.path.join(output_dir, f\"{regression_type}_{variable_name}_effect{prefix}.pdf\")\n",
    "    plot_variable_effect(\n",
    "        loss_data,\n",
    "        variable_values,\n",
    "        n_neighbors,\n",
    "        variable_name,\n",
    "        xlabel=\"Number of Neighbors ($k$)\",\n",
    "        ylabel=\"RMSE Loss\",\n",
    "        title=f\"{regression_type} ({variable_name.capitalize()} Effect)\",\n",
    "        save_path=plot_path,\n",
    "    )\n",
    "\n",
    "\n",
    "def process_and_plot_variable(\n",
    "    train_indices,\n",
    "    downsampled_data,\n",
    "    hyperparam_ranges,\n",
    "    variable_name,\n",
    "    output_dir,\n",
    "    regression_type,\n",
    "    generated_data_path,\n",
    "    label,\n",
    "    find_minimum=False\n",
    "):\n",
    "    # Generate or load results\n",
    "    loss_data, result_data, variable_values, n_neighbors = generate_results(\n",
    "        train_indices,\n",
    "        downsampled_data,\n",
    "        hyperparam_ranges,\n",
    "        variable_name,\n",
    "        generated_data_path,\n",
    "        label\n",
    "    )\n",
    "\n",
    "    # Plot results\n",
    "    plot_results_minimum(\n",
    "        loss_data,\n",
    "        variable_values,\n",
    "        n_neighbors,\n",
    "        variable_name,\n",
    "        output_dir,\n",
    "        regression_type,\n",
    "        find_minimum\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "47b10ed7-a9e6-4f75-b662-78190acf1137",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10 13 16 19 22]\n"
     ]
    }
   ],
   "source": [
    "# n_neighbors = np.logspace(0, 5.01, num=10, base=np.e).round().astype(np.int32)\n",
    "n_neighbors = np.asarray([10, 13, 16, 19, 22]).astype(np.int32)\n",
    "print(n_neighbors)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1927cedf-2603-4182-91ec-1299f613ee62",
   "metadata": {},
   "source": [
    "### Tau Variability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "12cc3092-bfe1-49dc-a797-8db3736bdae7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STELLAR MASS\n",
      "Loading cached results for tau = 0.15848931924611134\n",
      "Loading cached results for tau = 0.3511191734215131\n",
      "Loading cached results for tau = 0.7778737048694307\n",
      "Loading cached results for tau = 1.7233109056135083\n",
      "Loading cached results for tau = 3.817844026370506\n",
      "Loading cached results for tau = 8.458098281751338\n",
      "Loading cached results for tau = 18.73817422860384\n",
      "Loading cached results for tau = 41.51278002752293\n",
      "Loading cached results for tau = 91.96791985117059\n",
      "Loading cached results for tau = 203.74685280397102\n",
      "Loading cached results for tau = 451.3832659768975\n",
      "Loading cached results for tau = 1000.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_84262/1603112765.py:26: UserWarning: No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n",
      "  ax.legend(loc=\"upper right\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STELLAR METALLICITY\n",
      "Loading cached results for tau = 0.15848931924611134\n",
      "Loading cached results for tau = 0.3511191734215131\n",
      "Loading cached results for tau = 0.7778737048694307\n",
      "Loading cached results for tau = 1.7233109056135083\n",
      "Loading cached results for tau = 3.817844026370506\n",
      "Loading cached results for tau = 8.458098281751338\n",
      "Loading cached results for tau = 18.73817422860384\n",
      "Loading cached results for tau = 41.51278002752293\n",
      "Loading cached results for tau = 91.96791985117059\n",
      "Loading cached results for tau = 203.74685280397102\n",
      "Loading cached results for tau = 451.3832659768975\n",
      "Loading cached results for tau = 1000.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_84262/1603112765.py:26: UserWarning: No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n",
      "  ax.legend(loc=\"upper right\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STAR FORMATION\n",
      "Loading cached results for tau = 0.15848931924611134\n",
      "Loading cached results for tau = 0.3511191734215131\n",
      "Loading cached results for tau = 0.7778737048694307\n",
      "Loading cached results for tau = 1.7233109056135083\n",
      "Loading cached results for tau = 3.817844026370506\n",
      "Loading cached results for tau = 8.458098281751338\n",
      "Loading cached results for tau = 18.73817422860384\n",
      "Loading cached results for tau = 41.51278002752293\n",
      "Loading cached results for tau = 91.96791985117059\n",
      "Loading cached results for tau = 203.74685280397102\n",
      "Loading cached results for tau = 451.3832659768975\n",
      "Loading cached results for tau = 1000.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_84262/1603112765.py:26: UserWarning: No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n",
      "  ax.legend(loc=\"upper right\")\n"
     ]
    }
   ],
   "source": [
    "stellar_mass_hyperparam_ranges = {\n",
    "    \"p\": [1.0],\n",
    "    \"q\": [1.0],\n",
    "    \"tau\": taus, \n",
    "    \"alpha_C\": [14.672736961511486],\n",
    "    \"alpha_M\": [90.9960841116981],\n",
    "    \"alpha_SLB\": [56.26711116650133],\n",
    "    \"n_neighbors\": n_neighbors,\n",
    "}\n",
    "\n",
    "# Process and plot for Mass experiment\n",
    "print(\"STELLAR MASS\")\n",
    "process_and_plot_variable(\n",
    "    train_indices,\n",
    "    downsampled_data,\n",
    "    stellar_mass_hyperparam_ranges,\n",
    "    variable_name=\"tau\",\n",
    "    output_dir=\"mass_experiment_results\",\n",
    "    regression_type=\"Stellar Mass\",\n",
    "    generated_data_path=generated_data_path,\n",
    "    label=\"stellar_mass\",\n",
    "    find_minimum=False\n",
    ")\n",
    "\n",
    "\n",
    "# Process and plot for Metallicity experiment\n",
    "stellar_metallicity_hyperparam_ranges = {\n",
    "    \"p\": [1.0],\n",
    "    \"q\": [1.0],\n",
    "    \"tau\": taus,\n",
    "    \"alpha_C\": [41.38652023058744],\n",
    "    \"alpha_M\": [94.95040630340328],\n",
    "    \"alpha_SLB\": [70.89754616927755],\n",
    "    \"n_neighbors\": n_neighbors,\n",
    "}\n",
    "\n",
    "print(\"STELLAR METALLICITY\")\n",
    "process_and_plot_variable(\n",
    "    train_indices,\n",
    "    downsampled_data,\n",
    "    stellar_metallicity_hyperparam_ranges,\n",
    "    variable_name=\"tau\",\n",
    "    output_dir=\"metallicity_experiment_results\",\n",
    "    regression_type=\"Stellar Metallicity\",\n",
    "    generated_data_path=generated_data_path,\n",
    "    label=\"stellar_metallicity\",\n",
    "    find_minimum=False\n",
    ")\n",
    "\n",
    "print(\"STAR FORMATION\")\n",
    "# Process and plot for star formation rate experiment\n",
    "star_formation_rate_hyperparam_ranges = {\n",
    "    \"p\": [2.],\n",
    "    \"q\": [2.],\n",
    "    \"tau\": taus,\n",
    "    \"alpha_C\": [22.954853753245022],\n",
    "    \"alpha_M\": [94.96140687714441],\n",
    "    \"alpha_SLB\": [14.423051676594431],\n",
    "    \"n_neighbors\": n_neighbors\n",
    "}\n",
    "\n",
    "process_and_plot_variable(\n",
    "    train_indices,\n",
    "    downsampled_data,\n",
    "    star_formation_rate_hyperparam_ranges,\n",
    "    variable_name=\"tau\",\n",
    "    output_dir=\"star_formation_rate_experiment_results\",\n",
    "    regression_type=\"Star Formation Rate\",\n",
    "    generated_data_path=generated_data_path,\n",
    "    label=\"star_formation_rate\",\n",
    "    find_minimum=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "97845832-9a32-44a9-9a2d-70cf1e04956c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ray.shutdown()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
